{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib, csv, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlFile = 'TextFile/storyURL.txt' # read input\n",
    "termdoc_File = 'TextFile/Term_Document.csv'\n",
    "target_File = 'TextFile/Label_category.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract Contents/Title of story/blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContents(soup):\n",
    "    ignoreTags = ('a','script','noscript','em', 'iframe') # unwanted tag section\n",
    "    subtmp = str(soup)\n",
    "    for tg in ignoreTags:\n",
    "        if tg == 'a': sec_del = soup.find_all(tg, href='javascript:void(0);')\n",
    "        else: sec_del = soup.find_all(tg)\n",
    "        for sd in sec_del:\n",
    "            subtmp = subtmp.replace(str(sd), \"\")\n",
    "\n",
    "    newsec = BeautifulSoup(subtmp, 'lxml')\n",
    "    txtTags = ('p', 'h1', 'h2', 'h3', 'h4', 'th', 'li') # desired tag section\n",
    "    text = ''\n",
    "    for tg in txtTags:\n",
    "        if tg == 'p':\n",
    "            subs = newsec.findAll(tg, {\"class\":None}) # find 'p' tag with no class attr\n",
    "            if subs:\n",
    "                del subs[-1] # remove last 'p' section (Twitter)\n",
    "            else:\n",
    "                subs = ''\n",
    "        else:\n",
    "            subs = newsec.findAll(tg, {\"class\":re.compile('[^(button)]')}) # not word on Button\n",
    "        for ss in subs:\n",
    "            text = text + ss.get_text() + ' '\n",
    "#     text = newsec.get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# def getTitle(soup):\n",
    "#     head = soup.find('h1')\n",
    "#     pageTitle = head.get_text()\n",
    "#     return pageTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse Non-Printable Blog page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseNonPrintBlog(soup): # this is for PageType = 'blog'\n",
    "    sec = soup.select(\"div[id=blog-content] div[id=blog-left-pane] div[class=blog-post-content]\")\n",
    "    text = ''\n",
    "    if len(sec) >0: # single page\n",
    "        text += getContents(sec[0]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse Printable Story page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePrintStory(soup): # this is for PageType = 'story'\n",
    "    text = ''\n",
    "    sec = soup.select(\"div[class=slideshow-text]\") # multi-page story as slideshow\n",
    "    if len(sec) >0:\n",
    "        for ss in sec:  # multiple page\n",
    "            text += getContents(ss)\n",
    "    else:\n",
    "        ssec = soup.select(\"div[class*=storypage] div[class^=bgWhite]\")\n",
    "        if len(ssec) > 0:  # single page\n",
    "            text += getContents(ssec[0])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse to get target label 'Category', 'Adkeyword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractTarget(soup):\n",
    "\n",
    "    patternCategory = re.compile(\"PageManager\\.PageMetaData\\.Add\\('Category','([\\w, %\\./\\\\-]*)'\\)\", re.I)\n",
    "\n",
    "    parag = soup.head.find(text = patternCategory)\n",
    "    try:\n",
    "        m = re.search(patternCategory, parag)\n",
    "        if m:   \n",
    "            category = m.group(1)\n",
    "            category = wnl.lemmatize(category.strip())\n",
    "            category = stem.stem_word(category)\n",
    "            return category\n",
    "    except:\n",
    "        print 'No category found'\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'I\\'m', 'I am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    " ]\n",
    "class RegexpReplacer(object):\n",
    "   def __init__(self, patterns=replacement_patterns):\n",
    "\n",
    "      # Fixed this line - \"patterns\", not \"pattern\"\n",
    "      self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "   def replace(self, text):\n",
    "      s = text\n",
    "      for (pattern, repl) in self.patterns:\n",
    "          (s, count) = re.subn(pattern, repl, s)\n",
    "\n",
    "      return s\n",
    "replaceContract=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add bigram Tokens\n",
    "def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return [ngram for ngram in itertools.chain(words, bigrams)]\n",
    "\n",
    "num_string = 'NUMVAR'\n",
    "def stopwd_bigram_stem(content):\n",
    "    tokens = word_tokenize(content.lower())\n",
    "    \n",
    "    # Lemmanization\n",
    "    tokens = [wnl.lemmatize(word.strip()) for word in tokens]  \n",
    "\n",
    "    # remove stop words\n",
    "    tokens = [word for word in tokens if word not in english_stopwords]\n",
    "\n",
    "    # remove '.' in Acronym e.g. U.S -> US\n",
    "    tokens =  [re.sub(r'\\.', '', word) if re.search(r'([a-zA-Z]\\.)+', word) else word for word in tokens]\n",
    "    \n",
    "    # Convert digit number to 'NUMVAR' string\n",
    "#     tokens_tmp = []\n",
    "#     for word in tokens:\n",
    "#         if re.search(r'[a-zA-Z]+', word):\n",
    "#             tokens_tmp.append(word)\n",
    "#         elif re.search(r'\\d+', word):\n",
    "#             tokens_tmp.append(num_string)\n",
    "#     tokens = tokens_tmp\n",
    "    \n",
    "    # Add bigram Tokens\n",
    "    tokens = bigram_word_feats(tokens, n = int(len(tokens)/3))\n",
    "    \n",
    "    # stemming\n",
    "    tokens = [stem.stem_word(word) for word in tokens]\n",
    "\n",
    "    docTerm = list()\n",
    "    for tk in tokens:\n",
    "        \n",
    "        if type(tk) == tuple: wd = '__'.join(tk) # for a bi-gram\n",
    "        else: wd = tk\n",
    "\n",
    "#         if re.search(r'[a-zA-Z]+', wd): # contain alphabatic character, remain\n",
    "#             docTerm.append(wd)\n",
    "#         elif re.search(r'\\d+', wd): # contain number but not alphabatic\n",
    "#             if re.search(r'(19\\d{2})|(20\\d{2})'):\n",
    "#                 wd = 'YEAR'\n",
    "#             elif re.search\n",
    "\n",
    "        docTerm.append(wd)\n",
    "    docTerm = ' '.join(docTerm)\n",
    "    \n",
    "    return docTerm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read story URL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n",
      "No category found\n"
     ]
    }
   ],
   "source": [
    "docList = list()\n",
    "tagCategory_List = list()\n",
    "with open(urlFile, 'rb') as urlHandle:\n",
    "    urlReader = csv.reader(urlHandle, delimiter = ',', quotechar = \"'\")\n",
    "    for rnum, row in enumerate(urlReader):\n",
    "#         if (rnum >426) and (rnum<430): #break## for test run\n",
    "            url = row[0].strip()\n",
    "            print_url = re.sub('http://www.bankrate.com', \n",
    "                               'http://www.bankrate.com/system/util/print.aspx?p=', url)# use print_url to de-pagintion\n",
    "\n",
    "            response = urllib.urlopen(print_url)\n",
    "            content = ''         \n",
    "            if response.code == 200:   # printed-version story\n",
    "                html = response.read()\n",
    "                bsObj = BeautifulSoup(html, 'lxml')\n",
    "                content = parsePrintStory(bsObj.body)                    # extract page contents\n",
    "                bsoup = BeautifulSoup(urllib.urlopen(url).read(),'lxml')\n",
    "                tag_Category = extractTarget(bsoup)                  # extract page category\n",
    "            else:                      # non-printed version blog\n",
    "                requests = urllib.urlopen(url)\n",
    "                if requests.code == 200:\n",
    "                    html = requests.read()\n",
    "                    bsObj = BeautifulSoup(html,'lxml')\n",
    "                    content = parseNonPrintBlog(bsObj.body)              # extract page contents\n",
    "                    tag_Category = extractTarget(bsObj)             # extract page category\n",
    "\n",
    "    #         title = getTitle(bsObj) # extract page Title\n",
    "            content = replaceContract.replace(content) # replace contraction, e.g. I'll->I will\n",
    "            content_ps = stopwd_bigram_stem(content) # preprocessing stopword, bigram, stemming\n",
    "\n",
    "            docList.append(content_ps)\n",
    "            tagCategory_List.append(tag_Category)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term - Document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docTovect = CountVectorizer(max_df= 0.95, min_df= 2, strip_accents = 'ascii') # tokenization and count\n",
    "docTovect = TfidfVectorizer(max_df= 0.95, min_df= 2, strip_accents = 'ascii'   # tokenization and count\n",
    "#                             , norm = 'l2'\n",
    "#                             , sublinear_tf  = True\n",
    "                            ) \n",
    "\n",
    "docTovect_fit = docTovect.fit(docList)\n",
    "term_document = docTovect_fit.transform(docList)\n",
    "term_document_df = pd.DataFrame.from_records(term_document.toarray(), columns=docTovect.vocabulary_)\n",
    "tag_Category_df = pd.Series(np.asarray(tagCategory_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save data into .csv\n",
    "Handle1 = open(termdoc_File, 'wb')\n",
    "Handle2 = open(target_File, 'wb')\n",
    "term_document_df.to_csv(Handle1, index = False)\n",
    "tag_Category_df.to_csv(Handle2, index = False)\n",
    "Handle1.close()\n",
    "Handle2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.bankrate.com/finance/savings/tips/2014.aspx'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup = bsObj.body\n",
    "# ignoreTags = ('a','script','noscript','em', 'iframe') # unwanted tag section\n",
    "# newsec = ''\n",
    "# subtmp = str(soup)\n",
    "# for tg in ignoreTags:\n",
    "#     if tg == 'a': sec_del = soup.find_all(tg, href='javascript:void(0);')\n",
    "#     else: sec_del = soup.find_all(tg)\n",
    "#     for sd in sec_del:\n",
    "#         subtmp = subtmp.replace(str(sd), \"\")\n",
    "\n",
    "# # print soup\n",
    "# print subtmp\n",
    "# # sec_del = soup.find_all('a', href='javascript:void(0);')\n",
    "# # print subtmp.replace(str(sec_del[0]), \"\")\n",
    "url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
